{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyEhsa Demo - S√£o Paulo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(''), '..', 'src'))\n",
        "\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "from shapely.geometry import Point\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from pyehsa.emerging_hotspot_analysis import EmergingHotspotAnalysis\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: 150 observa√ß√µes, 25 locais\n"
          ]
        }
      ],
      "source": [
        "# Criar dados sint√©ticos - Grid 5x5 no centro de SP\n",
        "center_lat, center_lon = -23.5489, -46.6388\n",
        "step = 0.01\n",
        "\n",
        "data = []\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        lat = center_lat + (i - 2) * step\n",
        "        lon = center_lon + (j - 2) * step\n",
        "        location_id = f'SP_{i}_{j}'\n",
        "        \n",
        "        for month in range(6):\n",
        "            time_period = datetime(2024, 1, 1) + timedelta(days=30*month)\n",
        "            \n",
        "            value = np.random.poisson(10)\n",
        "            \n",
        "            # Hotspot emergente no canto superior direito\n",
        "            if i >= 3 and j >= 3 and month >= 2:\n",
        "                value += (month - 1) * 8\n",
        "            \n",
        "            value += np.random.normal(0, 2)\n",
        "            value = max(0, value)\n",
        "            \n",
        "            data.append({\n",
        "                'location_id': location_id,\n",
        "                'time_period': time_period,\n",
        "                'value': value,\n",
        "                'geometry': Point(lon, lat)\n",
        "            })\n",
        "\n",
        "gdf = gpd.GeoDataFrame(data, geometry='geometry', crs='EPSG:4326')\n",
        "print(f\"Dataset: {len(gdf)} observa√ß√µes, {gdf['location_id'].nunique()} locais\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>location_id</th>\n",
              "      <th>time_period</th>\n",
              "      <th>value</th>\n",
              "      <th>geometry</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SP_0_0</td>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>11.153114</td>\n",
              "      <td>POINT (-46.6588 -23.5689)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SP_0_0</td>\n",
              "      <td>2024-01-31</td>\n",
              "      <td>10.622500</td>\n",
              "      <td>POINT (-46.6588 -23.5689)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SP_0_0</td>\n",
              "      <td>2024-03-01</td>\n",
              "      <td>1.787107</td>\n",
              "      <td>POINT (-46.6588 -23.5689)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SP_0_0</td>\n",
              "      <td>2024-03-31</td>\n",
              "      <td>8.406927</td>\n",
              "      <td>POINT (-46.6588 -23.5689)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SP_0_0</td>\n",
              "      <td>2024-04-30</td>\n",
              "      <td>10.374283</td>\n",
              "      <td>POINT (-46.6588 -23.5689)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  location_id time_period      value                   geometry\n",
              "0      SP_0_0  2024-01-01  11.153114  POINT (-46.6588 -23.5689)\n",
              "1      SP_0_0  2024-01-31  10.622500  POINT (-46.6588 -23.5689)\n",
              "2      SP_0_0  2024-03-01   1.787107  POINT (-46.6588 -23.5689)\n",
              "3      SP_0_0  2024-03-31   8.406927  POINT (-46.6588 -23.5689)\n",
              "4      SP_0_0  2024-04-30  10.374283  POINT (-46.6588 -23.5689)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-09 10:04:31 - INFO - üöÄ Starting Emerging Hotspot Analysis\n",
            "2025-10-09 10:04:31 - INFO - üìä Input DataFrame shape: (150, 4)\n",
            "2025-10-09 10:04:31 - INFO - üéØ Analysis parameters:\n",
            "2025-10-09 10:04:31 - INFO -    - Region ID field: location_id\n",
            "2025-10-09 10:04:31 - INFO -    - Time period field: time_period\n",
            "2025-10-09 10:04:31 - INFO -    - Value field: value\n",
            "2025-10-09 10:04:31 - INFO -    - Random seed: 77\n",
            "2025-10-09 10:04:31 - INFO -    - Time lags (k): 1\n",
            "2025-10-09 10:04:31 - INFO -    - Simulations (nsim): 99\n",
            "2025-10-09 10:04:31 - INFO - üìà Data overview:\n",
            "2025-10-09 10:04:31 - INFO -    - Total rows: 150\n",
            "2025-10-09 10:04:31 - INFO -    - Unique regions: 25\n",
            "2025-10-09 10:04:31 - INFO -    - Unique time periods: 6\n",
            "2025-10-09 10:04:31 - INFO -    - Value range: [0.976, 50.745]\n",
            "2025-10-09 10:04:31 - INFO -    - Missing values in value: 0\n",
            "2025-10-09 10:04:31 - INFO - üîß Step 1: Validating and cleaning input data...\n",
            "2025-10-09 10:04:31 - INFO - ‚úÖ Data validation completed\n",
            "2025-10-09 10:04:31 - INFO - üó∫Ô∏è  Step 2: Processing spatial data...\n",
            "2025-10-09 10:04:31 - INFO - ‚úÖ Preprocessing completed: (150, 4)\n",
            "2025-10-09 10:04:31 - INFO -    - GeoDataFrame created with 150 records\n",
            "2025-10-09 10:04:31 - INFO - üî≤ Step 2.5: Creating complete spacetime cube...\n",
            "2025-10-09 10:04:31 - INFO - ‚úÖ Complete spacetime cube created: (150, 4)\n",
            "2025-10-09 10:04:31 - INFO - üîç Filtered out 0 rows with NaN values in 'value'\n",
            "2025-10-09 10:04:31 - INFO -    - Final dataset for analysis: (150, 4)\n",
            "2025-10-09 10:04:31 - INFO - üîÑ Sorting data to match R spacetime cube order...\n",
            "2025-10-09 10:04:31 - INFO -    - Data sorted by [time_period, location_id]\n",
            "2025-10-09 10:04:31 - INFO - üîó Step 3: Calculating spatial weights...\n",
            "2025-10-09 10:04:31 - INFO - ‚úÖ Spatial weights calculated in 0.02s\n",
            "2025-10-09 10:04:31 - INFO -    - Spatial weights matrix: 25 regions\n",
            "2025-10-09 10:04:31 - INFO -    - Average neighbors per region: 6.8\n",
            "2025-10-09 10:04:31 - INFO - üìä Step 4: Calculating Getis-Ord Gi* statistics (spacetime method)...\n",
            "2025-10-09 10:04:32 - INFO - ‚úÖ Gi* statistics calculated in 0.26s using spacetime method\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geometries are already Shapely objects, creating GeoDataFrame...\n",
            "Setting CRS...\n",
            "Creating complete spacetime cube:\n",
            "  - 25 locations\n",
            "  - 6 time periods\n",
            "  - 150 total combinations\n",
            "  - Original data: 150 rows\n",
            "  - Complete cube: 150 rows\n",
            "  - Missing combinations filled with NAs: 0\n",
            "Spacetime cube dimensions:\n",
            "  - 25 locations\n",
            "  - 6 time periods\n",
            "  - 150 total observations\n",
            "Gi* range: [-5.976, 6.604]\n",
            "Significant observations (p<=0.01): 23/150\n",
            "Significant observations (p<=0.05): 44/150\n",
            "DEBUG - Sample of borderline cases:\n",
            "  Obs 0: Gi*=0.187, p=0.2800, sig=False\n",
            "  Obs 10: Gi*=0.602, p=0.1900, sig=False\n",
            "  Obs 20: Gi*=0.197, p=0.1900, sig=False\n",
            "  Obs 30: Gi*=-1.530, p=0.0900, sig=False\n",
            "  Obs 40: Gi*=-1.047, p=0.1700, sig=False\n",
            "DEBUG - Neighbor permutation test:\n",
            "  Original neighbors: [[6, 5, 1, 0], [6, 0, 7, 5, 2, 1], [6, 1, 7, 8, 3, 2]]\n",
            "  Permuted neighbors: [[9, 1, 3, 6], [6, 7, 9, 2, 0, 5], [3, 7, 4, 9, 1, 5]]\n",
            "  Neighbor structure changed: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-09 10:04:32 - INFO -    - Output shape: (150, 7)\n",
            "2025-10-09 10:04:32 - INFO -    - Used 99 simulations for p-values\n",
            "2025-10-09 10:04:32 - INFO - üéØ Step 5: Performing emerging hotspot classification...\n",
            "2025-10-09 10:04:32 - INFO - ‚úÖ EHSA classification completed in 0.01s\n",
            "2025-10-09 10:04:32 - INFO -    - Results shape: (25, 8)\n",
            "2025-10-09 10:04:32 - INFO - ==================================================\n",
            "2025-10-09 10:04:32 - INFO - üìã ANALYSIS RESULTS SUMMARY\n",
            "2025-10-09 10:04:32 - INFO - ==================================================\n",
            "2025-10-09 10:04:32 - INFO - ‚è±Ô∏è  Total execution time: 0:00:00.322230\n",
            "2025-10-09 10:04:32 - INFO - üè∑Ô∏è  Classification distribution:\n",
            "2025-10-09 10:04:32 - INFO -    - no pattern detected: 15 regions (60.0%)\n",
            "2025-10-09 10:04:32 - INFO -    - consecutive hotspot: 5 regions (20.0%)\n",
            "2025-10-09 10:04:32 - INFO -    - sporadic coldspot: 4 regions (16.0%)\n",
            "2025-10-09 10:04:32 - INFO -    - sporadic hotspot: 1 regions (4.0%)\n",
            "2025-10-09 10:04:32 - INFO - üìä Mann-Kendall Tau statistics:\n",
            "2025-10-09 10:04:32 - INFO -    - Mean: -0.0240\n",
            "2025-10-09 10:04:32 - INFO -    - Std: 0.5288\n",
            "2025-10-09 10:04:32 - INFO -    - Range: [-0.6000, 0.8667]\n",
            "2025-10-09 10:04:32 - INFO - ==================================================\n",
            "2025-10-09 10:04:32 - INFO - ‚úÖ EHSA Analysis completed successfully!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total regions to process: 25\n",
            "   No pattern #1: SP_0_0, Gi* range [-2.02, 0.19], 0/10 significant\n",
            "   No pattern #2: SP_0_1, Gi* range [-3.22, -1.16], 0/10 significant\n",
            "‚úì SP_0_2: sporadic coldspot\n",
            "   No pattern #3: SP_0_3, Gi* range [-3.29, -1.14], 0/10 significant\n",
            "‚úì SP_1_1: sporadic coldspot\n",
            "‚úì SP_1_2: sporadic coldspot\n",
            "‚úì SP_1_3: sporadic coldspot\n",
            "‚úì SP_2_4: sporadic hotspot\n",
            "‚úì SP_3_3: consecutive hotspot\n",
            "‚úì SP_3_4: consecutive hotspot\n",
            "‚úì SP_4_2: consecutive hotspot\n",
            "‚úì SP_4_3: consecutive hotspot\n",
            "‚úì SP_4_4: consecutive hotspot\n",
            "Total results processed: 25\n"
          ]
        }
      ],
      "source": [
        "# Executar an√°lise EHSA\n",
        "results = EmergingHotspotAnalysis.emerging_hotspot_analysis(\n",
        "    gdf,\n",
        "    region_id_field='location_id',\n",
        "    time_period_field='time_period', \n",
        "    value='value',\n",
        "    k=1,\n",
        "    nsim=99\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padr√µes identificados:\n",
            "classification\n",
            "no pattern detected    15\n",
            "consecutive hotspot     5\n",
            "sporadic coldspot       4\n",
            "sporadic hotspot        1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Primeiros resultados:\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"['region_id'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(results[\u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m].value_counts())\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPrimeiros resultados:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mregion_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclassification\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtau\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mp_value\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.head()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"['region_id'] not in index\""
          ]
        }
      ],
      "source": [
        "# Mostrar resultados\n",
        "print(\"Padr√µes identificados:\")\n",
        "print(results['classification'].value_counts())\n",
        "print(\"\\nPrimeiros resultados:\")\n",
        "results.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'region_id'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[32m/var/folders/vw/lchjhtcn06jgcb946y77vpb80000gn/T/ipykernel_85729/4210225840.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Criar mapa\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m folium\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m locations = gdf[[\u001b[33m'location_id'\u001b[39m, \u001b[33m'geometry'\u001b[39m]].drop_duplicates()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m viz_data = results.merge(locations, left_on=\u001b[33m'region_id'\u001b[39m, right_on=\u001b[33m'location_id'\u001b[39m)\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m m = folium.Map(location=[center_lat, center_lon], zoom_start=\u001b[32m13\u001b[39m)\n\u001b[32m      8\u001b[39m \n",
            "\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10835\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m     ) -> DataFrame:\n\u001b[32m  10837\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10838\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m         return merge(\n\u001b[32m  10840\u001b[39m             self,\n\u001b[32m  10841\u001b[39m             right,\n\u001b[32m  10842\u001b[39m             how=how,\n",
            "\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
            "\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
            "\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1307\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1310\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1312\u001b[39m                         join_names.append(lk)\n\u001b[32m   1313\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
            "\u001b[32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
            "\u001b[31mKeyError\u001b[39m: 'region_id'"
          ]
        }
      ],
      "source": [
        "# Criar mapa\n",
        "import folium\n",
        "\n",
        "locations = gdf[['location_id', 'geometry']].drop_duplicates()\n",
        "viz_data = results.merge(locations, left_on=results.columns[0], right_on='location_id')\n",
        "\n",
        "m = folium.Map(location=[center_lat, center_lon], zoom_start=13)\n",
        "\n",
        "color_map = {\n",
        "    'no pattern detected': 'gray',\n",
        "    'new hotspot': 'red',\n",
        "    'consecutive hotspot': 'darkred',\n",
        "    'sporadic hotspot': 'orange'\n",
        "}\n",
        "\n",
        "for _, row in viz_data.iterrows():\n",
        "    color = color_map.get(row['classification'], 'gray')\n",
        "    \n",
        "    folium.CircleMarker(\n",
        "        location=[row['geometry'].y, row['geometry'].x],\n",
        "        radius=8,\n",
        "        popup=f\"{row[results.columns[0]]}: {row['classification']}\",\n",
        "        color='black',\n",
        "        fillColor=color,\n",
        "        fillOpacity=0.7\n",
        "    ).add_to(m)\n",
        "\n",
        "m\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
